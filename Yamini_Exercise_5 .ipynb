{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The fifth in-class-exercise (40 points in total, 4/18/2023)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
    "\n",
    "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
    "\n",
    "Algorithms:\n",
    "\n",
    "(1) MultinominalNB\n",
    "\n",
    "(2) SVM \n",
    "\n",
    "(3) KNN \n",
    "\n",
    "(4) Decision tree\n",
    "\n",
    "(5) Random Forest\n",
    "\n",
    "(6) XGBoost\n",
    "\n",
    "(7) Word2Vec\n",
    "\n",
    "(8) BERT\n",
    "\n",
    "Evaluation measurement:\n",
    "\n",
    "(1) Accuracy\n",
    "\n",
    "(2) Recall\n",
    "\n",
    "(3) Precison \n",
    "\n",
    "(4) F-1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: scipy in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.11.3)\n",
      "Requirement already satisfied: numpy in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.26.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading necessary packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "#load the dataset from files\n",
    "with open('stsa-train.txt', 'r') as f:\n",
    "    train_data = f.readlines()\n",
    "with open('stsa-test.txt', 'r') as f:\n",
    "    test_data = f.readlines()\n",
    "#split the labels and reviews\n",
    "train_labels = [int(line[0]) for line in train_data]\n",
    "train_reviews = [line[2:].strip() for line in train_data]\n",
    "test_labels = [int(line[0]) for line in test_data]\n",
    "test_reviews = [line[2:].strip() for line in test_data]\n",
    "#we have to vectorize the train and test data \n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_reviews)\n",
    "X_test = vectorizer.transform(test_reviews)\n",
    "#spliting the data for 80% and 20% for validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation Metrics:\n",
      "Accuracy: 0.8107\n",
      "Recall: 0.8485\n",
      "Precision: 0.7971\n",
      "F1-Score: 0.8220\n",
      "\n",
      "SVM Test Metrics:\n",
      "Accuracy: 0.7979\n",
      "Recall: 0.8196\n",
      "Precision: 0.7850\n",
      "F1-Score: 0.8019\n",
      "\n",
      "MultinomialNB Validation Metrics:\n",
      "Accuracy: 0.7970\n",
      "Recall: 0.8920\n",
      "Precision: 0.7571\n",
      "F1-Score: 0.8191\n",
      "\n",
      "MultinomialNB Test Metrics:\n",
      "Accuracy: 0.8023\n",
      "Recall: 0.8900\n",
      "Precision: 0.7568\n",
      "F1-Score: 0.8180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM and MultinomialNB Classifier\n",
    "svm = SVC(kernel='linear')\n",
    "mnb = MultinomialNB()\n",
    "# 10fold with SVM\n",
    "svm.fit(X_train_split, y_train_split)\n",
    "#10 fold with MultinomialNB\n",
    "mnb.fit(X_train_split, y_train_split)\n",
    "#evaluate on test data of svm\n",
    "svm_val_predictions = svm.predict(X_val)\n",
    "svm_test_predictions = svm.predict(X_test)\n",
    "#evaluate on tezt data of multinomialNB\n",
    "mnb_val_predictions = mnb.predict(X_val)\n",
    "mnb_test_predictions = mnb.predict(X_test)\n",
    "#calculate the metrics\n",
    "def print_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\\n\")\n",
    "# Output for SVM\n",
    "print_metrics(y_val, svm_val_predictions, \"SVM Validation\")\n",
    "print_metrics(test_labels, svm_test_predictions, \"SVM Test\")\n",
    "# MultinomialNB\n",
    "print_metrics(y_val, mnb_val_predictions, \"MultinomialNB Validation\")\n",
    "print_metrics(test_labels, mnb_test_predictions, \"MultinomialNB Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Validation Metrics:\n",
      "Accuracy: 0.7276\n",
      "Recall: 0.7882\n",
      "Precision: 0.7132\n",
      "F1-Score: 0.7488\n",
      "\n",
      "KNN Test Metrics:\n",
      "Accuracy: 0.7326\n",
      "Recall: 0.7811\n",
      "Precision: 0.7114\n",
      "F1-Score: 0.7446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_split, y_train_split)\n",
    "#evaluate on test data\n",
    "knn_val_predictions = knn.predict(X_val)\n",
    "knn_test_predictions = knn.predict(X_test)\n",
    "#calculate metrics\n",
    "def print_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\\n\")\n",
    "# printing output\n",
    "print_metrics(y_val, knn_val_predictions, \"KNN Validation\")\n",
    "print_metrics(test_labels, knn_test_predictions, \"KNN Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Validation Metrics:\n",
      "Accuracy: 0.6337\n",
      "Recall: 0.6802\n",
      "Precision: 0.6348\n",
      "F1-Score: 0.6567\n",
      "\n",
      "Decision Tree Test Metrics:\n",
      "Accuracy: 0.6178\n",
      "Recall: 0.6700\n",
      "Precision: 0.6060\n",
      "F1-Score: 0.6364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train_split, y_train_split)\n",
    "#evaluate on test data\n",
    "dt_val_pred = decision_tree.predict(X_val)\n",
    "dt_test_pred = decision_tree.predict(X_test)\n",
    "#initializing function to calculate metrics\n",
    "def print_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\\n\")\n",
    "# print output results\n",
    "print_metrics(y_val, dt_val_pred, \"Decision Tree Validation\")\n",
    "print_metrics(test_labels, dt_test_pred, \"Decision Tree Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation Metrics:\n",
      "Accuracy: 0.7233\n",
      "Recall: 0.7980\n",
      "Precision: 0.7042\n",
      "F1-Score: 0.7482\n",
      "\n",
      "Random Forest Test Metrics:\n",
      "Accuracy: 0.7100\n",
      "Recall: 0.7701\n",
      "Precision: 0.6869\n",
      "F1-Score: 0.7261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest classifier\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train_split, y_train_split)\n",
    "#evaluate on test data\n",
    "rf_val_pred = random_forest.predict(X_val)\n",
    "rf_test_pred = random_forest.predict(X_test)\n",
    "#calculate validation metrics\n",
    "accuracy_val = accuracy_score(y_val, rf_val_pred)\n",
    "recall_val = recall_score(y_val, rf_val_pred)\n",
    "precision_val = precision_score(y_val, rf_val_pred)\n",
    "f1_val = f1_score(y_val, rf_val_pred)\n",
    "#calculate text metrics\n",
    "accuracy_test = accuracy_score(test_labels, rf_test_pred)\n",
    "recall_test = recall_score(test_labels, rf_test_pred)\n",
    "precision_test = precision_score(test_labels, rf_test_pred)\n",
    "f1_test = f1_score(test_labels, rf_test_pred)\n",
    "#prinying output results \n",
    "print(\"Random Forest Validation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_val:.4f}\")\n",
    "print(f\"Recall: {recall_val:.4f}\")\n",
    "print(f\"Precision: {precision_val:.4f}\")\n",
    "print(f\"F1-Score: {f1_val:.4f}\\n\")\n",
    "\n",
    "print(\"Random Forest Test Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation Metrics:\n",
      "Accuracy: 0.7132\n",
      "Recall: 0.8135\n",
      "Precision: 0.6872\n",
      "F1-Score: 0.7450\n",
      "\n",
      "XGBoost Test Metrics:\n",
      "Accuracy: 0.7073\n",
      "Recall: 0.7855\n",
      "Precision: 0.6787\n",
      "F1-Score: 0.7282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGBoost classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_split, y_train_split)\n",
    "#evaluate on test data\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "#calculate  validation metrics\n",
    "accuracy_val = accuracy_score(y_val, xgb_val_pred)\n",
    "recall_val = recall_score(y_val, xgb_val_pred)\n",
    "precision_val = precision_score(y_val, xgb_val_pred)\n",
    "f1_val = f1_score(y_val, xgb_val_pred)\n",
    "#test metrics\n",
    "accuracy_test = accuracy_score(test_labels, xgb_test_pred)\n",
    "recall_test = recall_score(test_labels, xgb_test_pred)\n",
    "precision_test = precision_score(test_labels, xgb_test_pred)\n",
    "f1_test = f1_score(test_labels, xgb_test_pred)\n",
    "#printing output results for validation metrics\n",
    "print(\"XGBoost Validation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_val:.4f}\")\n",
    "print(f\"Recall: {recall_val:.4f}\")\n",
    "print(f\"Precision: {precision_val:.4f}\")\n",
    "print(f\"F1-Score: {f1_val:.4f}\\n\")\n",
    "#printing output results for test metrics\n",
    "print(\"XGBoost Test Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: transformers in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (4.34.0)\n",
      "Requirement already satisfied: sklearn in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (0.0.post11)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.26.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: requests in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: fsspec in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2022.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim transformers sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: transformers in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (4.34.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[K     |████████████████████████████████| 521 kB 929 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: aiohttp in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-macosx_10_9_x86_64.whl (31 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 30.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.1.2)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[K     |████████████████████████████████| 311 kB 50.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.8,>=0.3.0\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 56.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-macosx_10_14_x86_64.whl (26.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.9 MB 32.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Collecting tokenizers<0.15,>=0.14\n",
      "  Downloading tokenizers-0.14.0-cp39-cp39-macosx_10_7_x86_64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 37.5 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of safetensors to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.4.0-cp39-cp39-macosx_10_7_x86_64.whl (439 kB)\n",
      "  Downloading safetensors-0.3.3-cp39-cp39-macosx_10_11_x86_64.whl (404 kB)\n",
      "\u001b[K     |████████████████████████████████| 404 kB 24.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading safetensors-0.3.2-cp39-cp39-macosx_10_11_x86_64.whl (403 kB)\n",
      "\u001b[K     |████████████████████████████████| 403 kB 65.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading safetensors-0.3.1-cp39-cp39-macosx_10_11_x86_64.whl (400 kB)\n",
      "\u001b[K     |████████████████████████████████| 400 kB 51.2 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of idna to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "INFO: pip is looking at multiple versions of safetensors to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 25.1 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 30.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading idna-3.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading idna-3.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 28.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 32.2 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of idna to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 21.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading idna-2.7-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 34.9 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading idna-2.6-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 26.8 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading idna-2.5-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of certifi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 72.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 79.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 50.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
      "\u001b[K     |████████████████████████████████| 161 kB 97.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2022.9.14-py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 68.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2022.6.15.2-py3-none-any.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 29.9 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of certifi to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading certifi-2022.6.15.1-py3-none-any.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 175.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 36.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2022.5.18.1-py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 40.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 26.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "\u001b[K     |████████████████████████████████| 145 kB 85.6 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 151.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2020.11.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 139.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 65.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2020.4.5.2-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 170.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2020.4.5.1-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2020.4.5-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 40.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2019.11.28-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 27.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2019.9.11-py2.py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 24.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2019.6.16-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 22.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2019.3.9-py2.py3-none-any.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 48.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2018.11.29-py2.py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 40.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2018.10.15-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2018.8.24-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2018.8.13-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 40.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2018.4.16-py2.py3-none-any.whl (150 kB)\n",
      "\u001b[K     |████████████████████████████████| 150 kB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2018.1.18-py2.py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 55.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2017.11.5-py2.py3-none-any.whl (330 kB)\n",
      "\u001b[K     |████████████████████████████████| 330 kB 35.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2017.7.27.1-py2.py3-none-any.whl (349 kB)\n",
      "\u001b[K     |████████████████████████████████| 349 kB 38.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2017.7.27-py2.py3-none-any.whl (349 kB)\n",
      "\u001b[K     |████████████████████████████████| 349 kB 58.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading certifi-2017.4.17-py2.py3-none-any.whl (375 kB)\n",
      "\u001b[K     |████████████████████████████████| 375 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 7.5 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 324 kB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 7.9 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 260 kB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading requests-2.27.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 7.5 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 4.0 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 599 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "INFO: pip is looking at multiple versions of chardet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 102.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 30.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading chardet-3.0.3-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 41.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading chardet-3.0.2-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 42.9 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of chardet to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "Collecting requests\n",
      "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 157 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 37.5 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "Collecting requests\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 270 kB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 294 kB/s eta 0:00:01\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[K     |████████████████████████████████| 261 kB 933 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (1.13.0)\n",
      "Requirement already satisfied: psutil in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: pyyaml in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: huggingface-hub in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.4)\n",
      "Requirement already satisfied: typing_extensions in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: filelock in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (3.6.0)\n",
      "Requirement already satisfied: fsspec in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2022.2.0)\n",
      "Requirement already satisfied: requests in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.64.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.24.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#word2vec and BERT \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "#load the data\n",
    "with open('stsa-train.txt', 'r') as f:\n",
    "    train_data = f.readlines()\n",
    "\n",
    "with open('stsa-test.txt', 'r') as f:\n",
    "    test_data = f.readlines()\n",
    "train_labels = [int(line[0]) for line in train_data]\n",
    "train_reviews = [line[2:].strip() for line in train_data]\n",
    "test_labels = [int(line[0]) for line in test_data]\n",
    "test_reviews = [line[2:].strip() for line in test_data]\n",
    "\n",
    "\n",
    "#here we are loading pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "train_encodings = tokenizer(train_reviews, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_reviews, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#to evaluate we ate turn the labels into a PyTorch tensor\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "#custom dataset frm torch\n",
    "from torch.utils.data import Dataset\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "train_dataset = ReviewsDataset(train_encodings, train_labels)\n",
    "test_dataset = ReviewsDataset(test_encodings, test_labels)\n",
    "\n",
    "#here i am defining the compute_metrics function for the evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "#initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "#train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
    "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
    "(You can also use different text data which you want)\n",
    "\n",
    "Apply the listed clustering methods to the dataset:\n",
    "\n",
    "K-means\n",
    "\n",
    "DBSCAN\n",
    "\n",
    "Hierarchical clustering\n",
    "\n",
    "Word2Vec\n",
    "\n",
    "BERT\n",
    "\n",
    "You can refer to of the codes from  the follwing link below. \n",
    "https://www.kaggle.com/karthik3890/text-clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can write you answer here. (No code needed)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
